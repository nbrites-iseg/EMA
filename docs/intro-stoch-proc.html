<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Introduction to Stochastic Processes | Advanced Mathematical Economics (Part II)</title>
  <meta name="description" content="1 Introduction to Stochastic Processes | Advanced Mathematical Economics (Part II)" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Introduction to Stochastic Processes | Advanced Mathematical Economics (Part II)" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Introduction to Stochastic Processes | Advanced Mathematical Economics (Part II)" />
  
  
  

<meta name="author" content="Nuno M. Brites" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="sde.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#section" id="toc-section"></a></li>
<li class="chapter" data-level="1" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html"><i class="fa fa-check"></i><b>1</b> Introduction to Stochastic Processes</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#conceitos-fundamentais"><i class="fa fa-check"></i><b>1.1</b> Fundamental Concepts</a></li>
<li class="chapter" data-level="1.2" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#tipos-classicos-de-processos-estocasticos"><i class="fa fa-check"></i><b>1.2</b> Classical types of stochastic processes</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#processos-de-incrementos-independentes-e-estacionarios"><i class="fa fa-check"></i><b>1.2.1</b> Processes with independent and stationary increments</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#real-second-order-stochastic-process"><i class="fa fa-check"></i><b>1.2.2</b> Real Second-Order Stochastic Process</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#stationary-processes"><i class="fa fa-check"></i><b>1.2.3</b> Stationary Processes</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#martingales"><i class="fa fa-check"></i><b>1.2.4</b> Martingales</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro-stoch-proc.html"><a href="intro-stoch-proc.html#markov-processes"><i class="fa fa-check"></i><b>1.2.5</b> Markov Processes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sde.html"><a href="sde.html"><i class="fa fa-check"></i><b>2</b> Introduction to stochastic differential equations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="sde.html"><a href="sde.html#wiener-process"><i class="fa fa-check"></i><b>2.1</b> Wiener process</a></li>
<li class="chapter" data-level="2.2" data-path="sde.html"><a href="sde.html#itoint"><i class="fa fa-check"></i><b>2.2</b> The Itô integral</a></li>
<li class="chapter" data-level="2.3" data-path="sde.html"><a href="sde.html#itocalc-sde"><i class="fa fa-check"></i><b>2.3</b> Itô calculus and stochastic differential equations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><b>3</b> Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Mathematical Economics (Part II)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro-stoch-proc" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Introduction to Stochastic Processes<a href="intro-stoch-proc.html#intro-stoch-proc" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="conceitos-fundamentais" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Fundamental Concepts<a href="intro-stoch-proc.html#conceitos-fundamentais" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section provides a brief review of basic concepts in probability theory and random variables. We then introduce the notion of a <em>stochastic process</em>, understood as a family of random variables defined on a probability space and indexed by a parameter set, typically interpreted as time. Finally, we examine some fundamental classes of stochastic processes, in particular those with independent and stationary increments, as well as strictly and weakly stationary processes.</p>
<p>The <strong>sample space</strong> is denoted by <span class="math inline">\(\Omega\)</span> and represents the set of all possible outcomes of a random experiment. Throughout this section, we assume that <span class="math inline">\(\Omega\)</span> is a non-empty set.</p>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  (Sigma-algebra) </strong></span>A <span class="math inline">\(\sigma\)</span>-algebra is a collection <span class="math inline">\(\mathcal{F}\)</span> of subsets of <span class="math inline">\(\Omega\)</span> satisfying the following properties:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\emptyset \in \mathcal{F}\)</span> and <span class="math inline">\(\Omega \in \mathcal{F}\)</span>;</p></li>
<li><p>If <span class="math inline">\(A \in \mathcal{F}\)</span>, then <span class="math inline">\(A^c \in \mathcal{F}\)</span>, where <span class="math inline">\(A^c\)</span> denotes the complement of <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(\Omega\)</span>;</p></li>
<li><p>If <span class="math inline">\(A_n \in \mathcal{F}\)</span> for all <span class="math inline">\(n \in \mathbb{N}\)</span> and the sets are countable, then<br />
<span class="math display">\[
\bigcup_{n \in \mathbb{N}} A_n \in \mathcal{F}.
\]</span></p></li>
</ol>
<p>The elements of <span class="math inline">\(\mathcal{F}\)</span> are called measurable sets (or <span class="math inline">\(\mathcal{F}\)</span>-measurable sets to specify the underlying <span class="math inline">\(\sigma\)</span>-algebra).</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 1.2  (Probability Measure) </strong></span>A <strong>probability measure</strong> <span class="math inline">\(P\)</span> on the <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\(\mathcal{F}\)</span> is a function<br />
<span class="math display">\[
P: \mathcal{F} \rightarrow [0, 1]
\]</span><br />
satisfying the following properties:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(\emptyset) = 0\)</span>;</p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span>;</p></li>
<li><p>If <span class="math inline">\((A_n)_{n \in \mathbb{N}}\)</span> is a sequence of pairwise disjoint sets in <span class="math inline">\(\mathcal{F}\)</span>, then<br />
<span class="math display">\[
P\left(\bigcup_{n \in \mathbb{N}} A_n\right) = \sum_{n \in \mathbb{N}} P(A_n).
\]</span></p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.3  (Probability Space) </strong></span>A <strong>probability space</strong> is a triple <span class="math inline">\((\Omega, \mathcal{F}, P),\)</span> where:</p>
<ul>
<li><span class="math inline">\(\Omega\)</span> is a set (the sample space),</li>
<li><span class="math inline">\(\mathcal{F}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\Omega\)</span>,</li>
<li><span class="math inline">\(P\)</span> is a probability measure on <span class="math inline">\(\mathcal{F}\)</span>.</li>
</ul>
<p>The elements of <span class="math inline">\(\mathcal{F}\)</span> are called <strong>events</strong>. For any <span class="math inline">\(A \in \mathcal{F}\)</span>, the value <span class="math inline">\(P(A)\)</span> represents the probability of the event <span class="math inline">\(A\)</span>.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 1.4  (Borel Sigma-algebra) </strong></span>A <strong>Borel <span class="math inline">\(\sigma\)</span>-algebra</strong>, denoted <span class="math inline">\(\mathcal{B}\)</span>, defined on a set <span class="math inline">\(E\)</span>, satisfies the following properties:</p>
<ul>
<li><p><span class="math inline">\(\emptyset \in \mathcal{B}\)</span> and <span class="math inline">\(E \in \mathcal{B}\)</span>;</p></li>
<li><p><span class="math inline">\(\mathcal{B}\)</span> is closed under complementation: for all <span class="math inline">\(A \in \mathcal{B}\)</span>, we have <span class="math inline">\(A^c \in \mathcal{B}\)</span>;</p></li>
<li><p><span class="math inline">\(\mathcal{B}\)</span> is closed under countable unions: if <span class="math inline">\(A_i \in \mathcal{B}\)</span> for all <span class="math inline">\(i \in \mathbb{N}\)</span>, then<br />
<span class="math display">\[
\bigcup\limits_{i \in \mathbb{N}} A_i \in \mathcal{B}.
\]</span></p></li>
</ul>
<p>A Borel <span class="math inline">\(\sigma\)</span>-algebra is a specific example of a <span class="math inline">\(\sigma\)</span>-algebra and is typically associated with the open sets of <span class="math inline">\(E\)</span>. The most common Borel <span class="math inline">\(\sigma\)</span>-algebra is the one on <span class="math inline">\(\mathbb{R}\)</span>, denoted by <span class="math inline">\(\mathcal{B}_{\mathbb{R}}\)</span>, or simply <span class="math inline">\(\mathcal{B}\)</span> when there is no ambiguity.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.5  (Random Variable) </strong></span>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space. A function<br />
<span class="math display">\[
X: \Omega \rightarrow \mathbb{R}
\]</span><br />
is said to be a <strong>random variable (r.v.)</strong> if<br />
<span class="math display">\[
\forall ~ B \in \mathcal{B}: X^{-1}(B) \in \mathcal{F},
\]</span><br />
where <span class="math inline">\(\mathcal{B}\)</span> denotes the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>In addition, we say that <span class="math inline">\(X\)</span> is <strong><span class="math inline">\(\mathcal{F}\)</span>-measurable</strong>, or simply <strong>measurable</strong> when the <span class="math inline">\(\sigma\)</span>-algebra is understood from context.</p>
<p><img src="_main_files/figure-html/fig0-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-6" class="theorem"><strong>Theorem 1.1  </strong></span>Let <span class="math inline">\(X: \Omega \to \mathbb{R}\)</span> be a random variable. Define
<span class="math display">\[
\sigma(X) = \{ X^{-1}(B) : B \in \mathcal{B} \}.
\]</span>
Then, <span class="math inline">\(\sigma(X)\)</span> is the smallest <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\Omega\)</span> with respect to which <span class="math inline">\(X\)</span> is measurable. This <span class="math inline">\(\sigma\)</span>-algebra, which is contained in <span class="math inline">\(\mathcal{F}\)</span>, is called the <strong><span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(X\)</span></strong>.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 1.6  (Mean and Variance) </strong></span>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space, and let <span class="math inline">\(X: \Omega \rightarrow \mathbb{R}\)</span> be a random variable. The <strong>expected value</strong> (or <strong>mean</strong>) and the <strong>variance</strong> of <span class="math inline">\(X\)</span> are defined as follows:</p>
<p><strong>1. General case (with probability measure <span class="math inline">\(P\)</span>):</strong>
<span class="math display">\[
E(X) = \int_\Omega X \, dP, \quad
\operatorname{Var}(X) = \int_\Omega (X - E(X))^2 \, dP,
\]</span>
provided these integrals exist and are finite.</p>
<p><strong>2. Discrete case:</strong><br />
If <span class="math inline">\(X\)</span> takes values in a discrete set <span class="math inline">\(\{x_1, x_2, \dots\}\)</span> with probabilities <span class="math inline">\(p_i = P(X = x_i)\)</span>, then
<span class="math display">\[
E(X) = \sum_i x_i \, p_i, \quad
\operatorname{Var}(X) = \sum_i (x_i - E(X))^2 \, p_i.
\]</span></p>
<p><strong>3. Continuous case:</strong><br />
If <span class="math inline">\(X\)</span> has a probability density function <span class="math inline">\(f_X(x)\)</span> with respect to the Lebesgue measure, then
<span class="math display">\[
E(X) = \int_{-\infty}^{+\infty} x f_X(x) \, dx, \quad
\operatorname{Var}(X) = \int_{-\infty}^{+\infty} (x - E(X))^2 f_X(x) \, dx.
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>Definition 1.7  </strong></span>Let <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> be a probability space, and let <span class="math inline">\(X\)</span> be a random variable defined on this space.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X\)</span> is said to be a <strong>square-integrable random variable</strong> if<br />
<span class="math display">\[
E(X^2) &lt; +\infty;
\]</span></p></li>
<li><p>The space <strong><span class="math inline">\(L^2\)</span></strong> is the set of all square-integrable random variables defined on <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>;</p></li>
<li><p>The <strong><span class="math inline">\(L^2\)</span> norm</strong> is defined by
<span class="math display">\[
\forall ~ X \in L^2:~ \|X\|_{L^2} = \left(E(X^2)\right)^{1/2}.
\]</span></p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>Definition 1.8  </strong></span>Let <span class="math inline">\((X_n : n \in \mathbb{N})\)</span> be a sequence of random variables in <span class="math inline">\(L^2\)</span>. We say that <span class="math inline">\((X_n)\)</span> <strong>converges to <span class="math inline">\(X\)</span> in <span class="math inline">\(L^2\)</span></strong> if
<span class="math display">\[
\|X_n - X\|_{L^2} \rightarrow 0 \quad \text{as} \quad n \to +\infty,
\]</span>
or, equivalently,
<span class="math display">\[
E\left((X_n - X)^2\right) \to 0 \quad \text{as} \quad n \to +\infty.
\]</span></p>
<p>This type of convergence is called <strong>mean square convergence</strong>, and it is denoted by
<span class="math display">\[
X_n \xrightarrow{m.s.} X \quad \text{as} \quad n \to +\infty,
\]</span>
or
<span class="math display">\[
\mathop{l.i.m.}\limits_{n \to +\infty} X_n = X.
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 1.9  </strong></span>Let <span class="math inline">\(X\)</span> be a random variable and let <span class="math inline">\((X_n : n \in \mathbb{N})\)</span> be a sequence of random variables defined on the probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>We say that <span class="math inline">\(X_n\)</span> <strong>converges almost surely</strong> (a.s.) to <span class="math inline">\(X\)</span>, or that it <strong>converges with probability 1</strong>, denoted by<br />
<span class="math display">\[
X_n \xrightarrow{a.s.} X \quad \text{or} \quad \lim_{n \to +\infty} X_n = X \quad \text{a.s.},
\]</span><br />
if <span class="math inline">\(X_n(\omega) \to X(\omega)\)</span> for all <span class="math inline">\(\omega \in \Omega \setminus N\)</span>, where <span class="math inline">\(N \in \mathcal{F}\)</span> is a null set, i.e., <span class="math inline">\(P(N) = 0\)</span>.</p></li>
<li><p>We say that <span class="math inline">\(X_n\)</span> <strong>converges in probability</strong> (or <strong>stochastically</strong>) to <span class="math inline">\(X\)</span>, denoted by<br />
<span class="math display">\[
X_n \xrightarrow{P} X \quad \text{or} \quad P\text{-}\lim_{n \to +\infty} X_n = X,
\]</span><br />
if, for every <span class="math inline">\(\delta &gt; 0\)</span>,<br />
<span class="math display">\[
P(|X_n - X| &gt; \delta) \to 0 \quad \text{as} \quad n \to +\infty.
\]</span></p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<p>When studying phenomena that exhibit no temporal evolution, one typically uses <strong>random samples</strong> — that is, repetitions of i.i.d. observations (independent and identically distributed).</p>
<p>But what if we are dealing with <strong>random variables</strong> that have already been observed (or could have been) in the past and may be observed again in the future?</p>
<p>This is the case, for example, when studying:</p>
<ul>
<li><p>the daily price of a stock on the financial market;</p></li>
<li><p>the evolution of the unemployment rate over a given period;</p></li>
<li><p>the number of people arriving at a certain queue to be served;</p></li>
<li><p>the temperature over time at a specific location;</p></li>
<li><p><span class="math inline">\(\ldots\)</span></p></li>
</ul>
<p>In such cases, we typically have only a <strong>single realisation</strong> (called a <strong>trajectory</strong> or <strong>sample path</strong>) from which we seek to draw conclusions.</p>
<p>In this trajectory, observations are no longer independent.</p>
<p>Typical objectives include:</p>
<ul>
<li><p>forecasting future values;</p></li>
<li><p>identifying the nature of the underlying evolution;</p></li>
<li><p>filtering (i.e., prediction using partial observations).</p></li>
</ul>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 1.10  (Stochastic Process) </strong></span>A <strong>stochastic process</strong> (SP) is a family of random variables <span class="math inline">\(\{X_t, ~t \in T\}\)</span> defined on the same probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> and taking values in the same measurable space <span class="math inline">\((E, \mathcal{B})\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(T\)</span>: parameter space (or time);</p></li>
<li><p><span class="math inline">\(\Omega\)</span>: sample space;</p></li>
<li><p><span class="math inline">\(\mathcal{F}\)</span>: <span class="math inline">\(\sigma\)</span>-algebra defined on <span class="math inline">\(\Omega\)</span>;</p></li>
<li><p><span class="math inline">\(P\)</span>: probability measure;</p></li>
<li><p><span class="math inline">\(E\)</span>: state space (values taken by <span class="math inline">\(X\)</span>);</p></li>
<li><p><span class="math inline">\(\mathcal{B}\)</span>: Borel <span class="math inline">\(\sigma\)</span>-algebra defined on <span class="math inline">\(E\)</span>.</p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-12" class="remark"><em>Remark</em>. </span></p>
<ul>
<li><p>Given a probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> and an arbitrary set <span class="math inline">\(T\)</span>, a SP is a function <span class="math inline">\(X(t,\omega)\)</span> defined on <span class="math inline">\(T \times \Omega\)</span>, such that for each <span class="math inline">\(t \in T\)</span>, <span class="math inline">\(X_t(\omega)\)</span> is a random variable.</p></li>
<li><p>The concept of SP generalises that of a random variable by making it depend on a parameter <span class="math inline">\(t\)</span> with domain <span class="math inline">\(T\)</span>. Thus, a SP can be interpreted as an ordered family of random variables.</p></li>
<li><p>For each fixed <span class="math inline">\(\omega_0 \in \Omega\)</span>, <span class="math inline">\(X(\omega_0, t)\)</span> is a non-random function of <span class="math inline">\(t\)</span>. In this way, a SP can be identified with a system that assigns to each point <span class="math inline">\(\omega \in \Omega\)</span> a function of the parameter <span class="math inline">\(t\)</span>. Each of these functions is called a <strong>trajectory</strong> or <strong>realisation</strong> of the process <span class="math inline">\(X\)</span>.</p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>Definition 1.11  (Trajectory of a stochastic process) </strong></span>The <strong>trajectory</strong> or <strong>realisation</strong> of a stochastic process <span class="math inline">\(X\)</span> is the collection
<span class="math display">\[
\{X_t(\omega), ~ t \in T\}, \quad \forall ~ \omega \in \Omega.
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-14" class="remark"><em>Remark</em>. </span>In general, <span class="math inline">\((E, \mathcal{B}) = (\mathbb{R}^n, \mathcal{B}_{\mathbb{R}^n})\)</span>, where:</p>
<ul>
<li><p><span class="math inline">\(\mathbb{R}^n\)</span>: the set of possible values of the process <span class="math inline">\(X_t\)</span>;</p></li>
<li><p><span class="math inline">\(\mathcal{B}_{\mathbb{R}^n}\)</span>: the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}^n\)</span>;</p></li>
<li><p>If <span class="math inline">\(n=1\)</span>, the SP is called a <strong>univariate stochastic process</strong>;</p></li>
<li><p>If <span class="math inline">\(n &gt; 1\)</span>, the SP is called a <strong>multivariate stochastic process</strong>;</p></li>
<li><p><span class="math inline">\(t\)</span>: the instant at which the observation is made or the time period relative to that observation;</p></li>
<li><p>If <span class="math inline">\(E\)</span> is finite or countably infinite, then <span class="math inline">\(X\)</span> is a <strong>discrete state space stochastic process</strong>;</p></li>
<li><p>If <span class="math inline">\(E = \mathbb{R}\)</span>, then <span class="math inline">\(X\)</span> is a <strong>real-valued stochastic process</strong>;</p></li>
<li><p>If <span class="math inline">\(T\)</span> is finite or countably infinite, then <span class="math inline">\(X\)</span> is a <strong>discrete time stochastic process</strong> (typically <span class="math inline">\(T = \mathbb{N}_0\)</span> or <span class="math inline">\(T = \mathbb{Z}\)</span>);</p></li>
<li><p>If <span class="math inline">\(T\)</span> is uncountably infinite, then <span class="math inline">\(X\)</span> is a <strong>continuous time stochastic process</strong> (typically <span class="math inline">\(T = \mathbb{R}^+_0\)</span> or <span class="math inline">\(T = \mathbb{R}\)</span>).</p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<p>Below is an example of a trajectory of a stochastic process:</p>
<p><img src="_main_files/figure-html/simulacao-movimento-browniano-1.png" width="672" /></p>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-15" class="exercise"><strong>Exercise 1.1  </strong></span>For each of the following stochastic processes, indicate the parameter space and the state space:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Let <span class="math inline">\(X_i\)</span> be the amount of beer (in litres) ordered by the <span class="math inline">\(i\)</span>-th customer entering a bar, and let <span class="math inline">\(N(t)\)</span> be the number of customers who have arrived at the bar by time <span class="math inline">\(t\)</span>. The stochastic process is
<span class="math display">\[
Z_t = \sum\limits_{i=1}^{N(t)} X_i, \quad t \geq 0,
\]</span>
where <span class="math inline">\(Z_t\)</span> represents the total amount of beer ordered up to time <span class="math inline">\(t\)</span>.</p></li>
<li><p>A baby sleeps in one of three positions: (i) lying on their back with a radiant expression; (ii) curled up in the fetal position; (iii) in the fetal position sucking their thumb. Let <span class="math inline">\(X_t\)</span> be the baby’s sleeping position at time <span class="math inline">\(t\)</span>. The process is <span class="math inline">\((X_t: \quad t \geq 0)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_n\)</span> be the state (on or off) of an office photocopier at noon on the <span class="math inline">\(n\)</span>-th day. The process is <span class="math inline">\((X_n: \quad n = 1, 2, \dots)\)</span>.</p></li>
</ol>
</div>
</div>
<div id="tipos-classicos-de-processos-estocasticos" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Classical types of stochastic processes<a href="intro-stoch-proc.html#tipos-classicos-de-processos-estocasticos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="processos-de-incrementos-independentes-e-estacionarios" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Processes with independent and stationary increments<a href="intro-stoch-proc.html#processos-de-incrementos-independentes-e-estacionarios" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-16" class="definition"><strong>Definition 1.12  (Process with independent increments) </strong></span><span class="math inline">\(\{X_t, ~ t \in T\}\)</span> is a stochastic process with <strong>independent increments</strong> if and only if
<span class="math display">\[
\forall ~ n \in \mathbb{N}, \forall ~ t_1, \ldots, t_n \in T: ~ t_1 &lt; t_2 &lt; \ldots &lt; t_n \implies X_{t_2} - X_{t_1}, X_{t_3} - X_{t_2}, \ldots, X_{t_n} - X_{t_{n-1}}
\]</span>
are mutually independent random variables.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 1.13  (Process with stationary increments) </strong></span><span class="math inline">\(\{X_t, ~ t \in T\}\)</span> has <strong>stationary increments</strong> if and only if for all <span class="math inline">\(s, t \in T\)</span>, with <span class="math inline">\(s &lt; t,\)</span> the distribution of <span class="math inline">\(X_t - X_s\)</span> depends only on the length <span class="math inline">\(t - s\)</span>.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-18" class="remark"><em>Remark</em>. </span>In a stochastic process with stationary increments, the distribution of <span class="math inline">\(X_{t_1 + h} - X_{t_1}\)</span> is the same as that of
<span class="math inline">\(X_{t_2 + h} - X_{t_2}\)</span>, for all <span class="math inline">\(t_1, t_2 \in T\)</span> and for all <span class="math inline">\(h \in \mathbb{R}_0^+\)</span> such that <span class="math inline">\(t_1 + h, t_2 + h \in T\)</span>.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-19" class="definition"><strong>Definition 1.14  (Process with independent and stationary increments) </strong></span>Given a stochastic process (SP) <span class="math inline">\(X := \{X_t, ~ t \in T\}\)</span>, where <span class="math inline">\(T\)</span> is equipped with an order relation, <span class="math inline">\(X\)</span> is a process with <strong>independent and stationary increments</strong> if and only if it has independent increments and stationary increments.</p>
</div>
</div>
<div id="real-second-order-stochastic-process" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Real Second-Order Stochastic Process<a href="intro-stoch-proc.html#real-second-order-stochastic-process" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-20" class="definition"><strong>Definition 1.15  (Gaussian Process) </strong></span>A stochastic process <span class="math inline">\(\{X_t, ~t \in T\}\)</span> is called a <strong>Gaussian Process</strong> if
<span class="math display">\[
\forall ~n \in \mathbb{N},~ \forall ~t_1, \ldots, t_n \in T, \quad (X_{t_1}, X_{t_2}, \ldots, X_{t_n}) \sim \mathcal{N}_n(\mu, \Sigma),
\]</span>
that is, any finite vector of random variables from the process has a multivariate normal distribution.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 1.16  (Real Second-Order Stochastic Process) </strong></span>A stochastic process <span class="math inline">\(\{X_t, ~ t \in T\}\)</span> is called a <strong>real second-order stochastic process</strong> if, and only if,
<span class="math display">\[
\forall ~t \in T: \; E\!\left(X_t^2\right) &lt; +\infty.
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 1.1  (Gaussian White Noise) </strong></span>A <strong>Gaussian White Noise</strong> process <span class="math inline">\(\{\varepsilon_t, ~t \in T\}\)</span> is defined as a stochastic process that satisfies:</p>
<ul>
<li><p><span class="math inline">\(\forall ~t \in T, ~E(\varepsilon_t)=0\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~t \in T, ~\mathrm{Var}(\varepsilon_t)=\sigma^2\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~s, t \in T, s \neq t, ~\mathrm{Cov}(\varepsilon_s,\varepsilon_t)=0\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~n \in \mathbb{N}, \forall ~t_1, t_2, \ldots, t_n \in T\)</span>, the vector <span class="math inline">\((\varepsilon_{t_1}, \varepsilon_{t_2}, \ldots, \varepsilon_{t_n})\)</span> is Gaussian.</p></li>
</ul>
</div>
</div>
<div id="stationary-processes" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Stationary Processes<a href="intro-stoch-proc.html#stationary-processes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>Definition 1.17  (Strictly Stationary Process) </strong></span>A stochastic process <span class="math inline">\(\{X_t,~ t \in T\}\)</span> is said to be <strong>strictly stationary</strong> (or strongly stationary) if:
<span class="math display">\[
\forall~n \in \mathbb{N},~ \forall~t_1, \ldots, t_n \in T,~ \forall~h \in \mathbb{R} \text{ such that } t_1 + h, \ldots, t_n + h \in T,
\]</span>
<span class="math display">\[
(X_{t_1}, \ldots, X_{t_n}) \stackrel{d}{=} (X_{t_1+h}, \ldots, X_{t_n+h}),
\]</span>
that is, the joint distribution of any finite vector of random variables of the process is invariant under time shift.</p>
</div>
<p>As a consequence of strict stationarity, we have the following theorem:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 1.2  </strong></span>If <span class="math inline">\(\{X_t, t \in T\}\)</span> is a second-order stochastic process and is strictly stationary, then:</p>
<ul>
<li><p><span class="math inline">\(E(X_t) = m\)</span>, that is, the mean of the process is independent of <span class="math inline">\(t\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~h \in T, ~ \Gamma(t,t+h) = \operatorname{Cov}(X_t, X_{t+h}) = \operatorname{Cov}(X_0, X_h) = \gamma(h)\)</span>,
independent of <span class="math inline">\(t\)</span>.</p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 1.18  (Weakly Stationary Process) </strong></span>A stochastic process <span class="math inline">\(\{X_t, t \in T\}\)</span> is <strong>weakly stationary</strong> (or second-order stationary) if and only if:</p>
<ul>
<li><p><span class="math inline">\(\forall ~t \in T, ~ E(X_t^2) &lt; +\infty\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~t \in T, ~ E(X_t) = m\)</span>, independent of <span class="math inline">\(t\)</span>;</p></li>
<li><p><span class="math inline">\(\forall ~t \in T, \forall ~h \in T, ~ \operatorname{Cov}(X_t, X_{t+h}) = \gamma(h)\)</span>, i.e., the covariance depends only on <span class="math inline">\(h\)</span>.</p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-26" class="remark"><em>Remark</em>. </span>The function <span class="math inline">\(\gamma(h), ~ \forall ~ h \in T\)</span>, is called the <strong>autocovariance function</strong>. If <span class="math inline">\(h=0\)</span>, then
<span class="math display">\[
\operatorname{Cov}(X_t, X_{t+h}) = \operatorname{Var}(X_t) = \gamma(0), \quad \forall ~ t \in T.
\]</span>
This property is called <strong>homoscedasticity</strong>.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<p>Now, let’s see that White Noise, <span class="math inline">\(\{\varepsilon_t, ~ t \in T\}\)</span>, is an example of a second-order stationary stochastic process:</p>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 1.2  </strong></span></p>
<ul>
<li><p><span class="math inline">\(E(\varepsilon_t) = 0\)</span>;</p></li>
<li><p><span class="math inline">\(Var(\varepsilon_t) = \sigma^2 \implies E(\varepsilon_t^2) &lt; + \infty\)</span>;</p></li>
<li><p>For <span class="math inline">\(t \neq s\)</span>, <span class="math inline">\(Cov(\varepsilon_s, \varepsilon_t) = 0 \implies\)</span> independence between <span class="math inline">\(t\)</span> and <span class="math inline">\(s\)</span>.</p></li>
</ul>
<p>Thus,</p>
<p><span class="math display">\[
\gamma(h) =
\begin{cases}
\sigma^2, &amp; h = 0, \\
0, &amp; h \neq 0.
\end{cases}
\]</span></p>
<p>Hence, the conditions for weak stationarity are satisfied.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-28" class="remark"><em>Remark</em> (Important remark). </span><span class="math display">\[\text{Strong stationarity} + E(X_t^2) &lt; +\infty \Rightarrow \text{Weak stationarity}.\]</span>
<span class="math display">\[\text{Weak stationarity} \nRightarrow \text{Strong stationarity}.\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 1.3  </strong></span>Consider the stochastic process <span class="math inline">\((X_t, ~ t \in \mathbb{N})\)</span> where <span class="math inline">\(X_t\)</span> has a Cauchy distribution, i.e., with probability density function
<span class="math display">\[
f(x) = \frac{1}{\pi(1 + x^2)}.
\]</span>
Since <span class="math inline">\(E(X_t)\)</span> does not exist, then <span class="math inline">\(E(X_t^2)\)</span> is not defined. Thus, the process is strongly stationary but not weakly stationary.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-30" class="definition"><strong>Definition 1.19  (Autocorrelation function in stationary processes) </strong></span>Let <span class="math inline">\(\{X_t, ~ t \in T\}\)</span> be a stationary stochastic process. The <strong>autocorrelation function</strong> <span class="math inline">\(\rho\)</span> is defined by:
<span class="math display">\[
\rho(h) = Corr(X_t, X_{t+h}) = \frac{Cov(X_t, X_{t+h})}{\sqrt{Var(X_t)} \sqrt{Var(X_{t+h})}} = \frac{\gamma(h)}{\gamma(0)}.
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-31" class="exercise"><strong>Exercise 1.2  </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with zero mean, uncorrelated, and with the same variance <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Consider the stochastic process <span class="math inline">\((Z_t: ~ t \in \mathbb{Z})\)</span> defined by:</p>
<p><span class="math display">\[
Z_t = f(t) \cdot X + g(t) \cdot Y, \quad t \in \mathbb{Z},
\]</span></p>
<p>where <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are deterministic functions.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find expressions for <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> so that the process <span class="math inline">\((Z_t: ~ t \in \mathbb{Z})\)</span> has constant variance but is not necessarily weakly stationary.</p></li>
<li><p>Specify <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> such that <span class="math inline">\((Z_t: ~ t \in \mathbb{Z})\)</span> is weakly stationary.</p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-32" class="exercise"><strong>Exercise 1.3  </strong></span>Let <span class="math inline">\(\varepsilon = (\varepsilon_t: ~ t \in \mathbb{Z})\)</span> be a white noise process with variance <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Consider the stochastic processes <span class="math inline">\(X = (X_t: ~ t \in \mathbb{Z})\)</span> and <span class="math inline">\(Y = (Y_t: ~ t \in \mathbb{Z})\)</span> defined by:</p>
<p><span class="math display">\[
X_t = \varepsilon_t \quad \text{and} \quad Y_t = (-1)^t \varepsilon_t, \quad \forall ~ t \in \mathbb{Z}.
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Prove that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are weakly stationary.</p></li>
<li><p>Show that the process <span class="math inline">\((Z_t = X_t + Y_t: ~ t \in \mathbb{Z})\)</span> is a non-stationary process.</p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-33" class="exercise"><strong>Exercise 1.4  </strong></span>Consider a stochastic process <span class="math inline">\(Y = (Y_t: t \in \mathbb{Z})\)</span> such that
<span class="math display">\[Y_t = \varepsilon_t - \theta \varepsilon_{t-1}, \quad \theta \in [-1,1],\]</span>
where <span class="math inline">\((\varepsilon_t: t \in \mathbb{Z})\)</span> is a Gaussian white noise with variance <span class="math inline">\(\sigma^2 &gt; 0\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that <span class="math inline">\(Y\)</span> is Gaussian.</p></li>
<li><p>Determine the distribution of the random variable <span class="math inline">\(Y_t\)</span>, for all <span class="math inline">\(t \in \mathbb{Z}\)</span>.</p></li>
<li><p>Determine the autocorrelation function of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>What can you conclude about the strong and weak stationarity of <span class="math inline">\(Y\)</span>?</p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>Exercise 1.5  </strong></span>Let <span class="math inline">\(X = (X_t: ~ t \geq 0)\)</span> be a stochastic process defined on the probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span> such that, for every <span class="math inline">\(t \geq 0\)</span>, <span class="math inline">\(X_t \sim \mathcal{N}(0, t)\)</span> and <span class="math inline">\(P(X_0 = 0) = 1\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Under what conditions is <span class="math inline">\(X\)</span> a process with independent and stationary increments?</p></li>
<li><p>Assuming <span class="math inline">\(X\)</span> is a process with independent and stationary increments, show that:</p></li>
<li><p>For all <span class="math inline">\(t, s \in [0,+\infty)\)</span> with <span class="math inline">\(t &gt; s\)</span>, it holds that
<span class="math display">\[X_t - X_s \sim \mathcal{N}(0, |t - s|);\]</span></p></li>
</ol>
<ol start="2" style="list-style-type: lower-roman">
<li><p><span class="math inline">\(X\)</span> is a centered Gaussian process.</p></li>
<li><p>Consider the stochastic process <span class="math inline">\(Y = (Y_t: t \geq 0)\)</span> defined by:
<span class="math display">\[
Y(t) =
\begin{cases}
t, &amp; \text{if } X_t \geq 0, \\
-t, &amp; \text{if } X_t &lt; 0.
\end{cases}
\]</span>
Show that <span class="math inline">\(Y\)</span> is a centered second-order stochastic process. Is <span class="math inline">\(Y\)</span> stationary in any sense? Justify your answer.</p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-35" class="exercise"><strong>Exercise 1.6  </strong></span>Let <span class="math inline">\(X = (X_t: ~t \in \mathbb{Z})\)</span> and <span class="math inline">\((\varepsilon_t: ~t \in \mathbb{Z})\)</span> be two stochastic processes defined on the probability space <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, such that:
<span class="math display">\[
\forall ~t \in \mathbb{Z}, \quad X_t = \sum\limits_{j=0}^{+\infty} \left( \frac{4}{5} \right)^j \varepsilon_{t-j}.
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Explain under which conditions <span class="math inline">\(\varepsilon\)</span> is a white noise.</p></li>
<li><p>Suppose that <span class="math inline">\(\varepsilon\)</span> is a white noise such that <span class="math inline">\(E[\varepsilon_t^2] = 9/50\)</span>.</p></li>
<li><p>Prove that <span class="math inline">\(X\)</span> is weakly stationary and indicate the corresponding mean function and autocovariance function;</p></li>
</ol>
<ol start="2" style="list-style-type: lower-roman">
<li><p>Now suppose that <span class="math inline">\(X\)</span> is a Gaussian process. Specify the distribution of the random vector <span class="math inline">\((X_t, X_s), ~ \forall ~ t, s \in \mathbb{Z}\)</span>.</p></li>
<li><p>Consider the stochastic process <span class="math inline">\(Y = (Y_t: t \in \mathbb{Z})\)</span> defined by:
<span class="math display">\[
Y_t =
\begin{cases}
\frac{1}{2}, &amp; \text{if } X_t \geq 0, \\
-1, &amp; \text{if } X_t &lt; 0,
\end{cases}
\]</span>
assuming that <span class="math inline">\(X\)</span> satisfies the conditions in item (b) ii). Calculate the mean function of <span class="math inline">\(Y\)</span> and show that <span class="math inline">\(Y\)</span> is weakly stationary.</p></li>
</ol>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-36" class="exercise"><strong>Exercise 1.7  </strong></span>Let <span class="math inline">\((\varepsilon_t: t \in \mathbb{Z})\)</span> be a Gaussian white noise with variance <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Consider another stochastic process <span class="math inline">\((Y_t: ~t \in \mathbb{Z})\)</span> defined by:
<span class="math display">\[
Y_t = \varepsilon_t - \theta \varepsilon_{t-1} - \frac{\theta}{2} \varepsilon_{t-2}, \quad \theta \in [-1,1].
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Define a Gaussian process and show that <span class="math inline">\(Y\)</span> is Gaussian.</p></li>
<li><p>Determine the autocorrelation function of the process <span class="math inline">\(Y\)</span>.</p></li>
</ol>
</div>
</div>
<div id="martingales" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Martingales<a href="intro-stoch-proc.html#martingales" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From a modeling perspective, martingales are suitable for modeling random phenomena such as gambling.</p>
<div class="definition">
<p><span id="def:unlabeled-div-37" class="definition"><strong>Definition 1.20  (Martingale) </strong></span>A stochastic process <span class="math inline">\(\{X_t, ~ t \in T\}\)</span> is a <strong>Martingale</strong> if and only if:</p>
<ul>
<li><p><span class="math inline">\(E(|X_t|) &lt; +\infty\)</span>;</p></li>
<li><p>For all <span class="math inline">\(n \in \mathbb{N}\)</span>, for all <span class="math inline">\(t_1 &lt; \ldots &lt; t_{n+1} \in T\)</span>:
<span class="math display">\[
E(X_{t_{n+1}} \mid X_{t_1}, \ldots, X_{t_n}) = X_{t_n}.
\]</span></p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-38" class="remark"><em>Remark</em>. </span>In the definition of Martingale, we can also consider:</p>
<ul>
<li><p>Submartingales, when for all <span class="math inline">\(n \in \mathbb{N}\)</span> and for all <span class="math inline">\(t_1 &lt; \ldots &lt; t_{n+1} \in T\)</span>:
<span class="math display">\[
E(X_{t_{n+1}} \mid X_{t_1}, \ldots, X_{t_n}) \leq X_{t_n}.
\]</span></p></li>
<li><p>Supermartingales, when for all <span class="math inline">\(n \in \mathbb{N}\)</span> and for all <span class="math inline">\(t_1 &lt; \ldots &lt; t_{n+1} \in T\)</span>:
<span class="math display">\[
E(X_{t_{n+1}} \mid X_{t_1}, \ldots, X_{t_n}) \geq X_{t_n}.
\]</span></p></li>
</ul>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-39" class="exercise"><strong>Exercise 1.8  </strong></span></p>
<!-- Muller pg 190 -->
<p>Let <span class="math inline">\(X_0, X_1, \dots\)</span> be independent random variables with finite zero mean and define <span class="math inline">\(S_n = \sum_{i=0}^n X_i\)</span>. Show that the stochastic process <span class="math inline">\(\{S_n: n \in \mathbb{N}_0\}\)</span> is a Martingale.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-40" class="exercise"><strong>Exercise 1.9  </strong></span></p>
<!-- Muller pg 191 -->
<p>Consider a game where in each round the player can win or lose one euro, with equal probability. After <span class="math inline">\(n\)</span> rounds, the player’s gain is given by <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span>, where <span class="math inline">\(X_1, X_2, \dots\)</span> are independent random variables. Show that the stochastic process <span class="math inline">\(\{S_n: n \in \mathbb{N}\}\)</span> is a Martingale.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-41" class="exercise"><strong>Exercise 1.10  </strong></span></p>
<!-- Muller pg 191 -->
<p>Let <span class="math inline">\(X_1, X_2, \dots\)</span> be independent random variables with mean one. Show that the stochastic process <span class="math inline">\(\{Z_n: n \in \mathbb{N}\}\)</span> defined by
<span class="math display">\[
Z_n = \prod_{i=1}^n X_i
\]</span>
is a Martingale.</p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-42" class="exercise"><strong>Exercise 1.11  </strong></span>Let <span class="math inline">\((X_n, n=0,1,2,\dots)\)</span> be a stochastic process with state space <span class="math inline">\(\mathbb{N}_0\)</span>, with mean one for <span class="math inline">\(n \geq 1\)</span>, independent increments, and such that <span class="math inline">\(P(X_0=0) = 1\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>What does it mean to say that the process <span class="math inline">\(X\)</span> has independent increments?</p></li>
<li><p>Prove that the process <span class="math inline">\((X_n, n=0,1,2,\dots)\)</span> is a Martingale.</p></li>
<li><p>Given that <span class="math inline">\(Var(X_n) = 1\)</span>, what can be said about the weak stationarity of the process <span class="math inline">\((X_n, n=0,1,2,\dots)\)</span>?</p></li>
</ol>
</div>
</div>
<div id="markov-processes" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Markov Processes<a href="intro-stoch-proc.html#markov-processes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Markov processes are suitable for modeling random phenomena whose future behavior is not influenced by the knowledge of their past, but only depends on the current state. In other words, the probability that the physical system is in a given state at time <span class="math inline">\(t\)</span> can be deduced from the knowledge of that state at any previous time, and this probability does not depend on the “history” of the system before <span class="math inline">\(t\)</span>.</p>
<p><span class="math inline">\(\,\)</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-43" class="definition"><strong>Definition 1.21  (Markov Process) </strong></span>A stochastic process <span class="math inline">\(\{X_t, t \in T\}\)</span> with state space <span class="math inline">\(E\)</span> is called a <strong>Markov process</strong> (or <strong>Markovian</strong>) if and only if for all <span class="math inline">\(n \in \mathbb{N}\)</span>, for all <span class="math inline">\(t_1 &lt; \ldots &lt; t_{n+1} \in T\)</span>, for all <span class="math inline">\(x_1, \ldots, x_{n+1} \in E\)</span>, and for all <span class="math inline">\(B \in \mathcal{B}\)</span>:
<span class="math display">\[
P(X_{t_{n+1}} \in B \mid X_{t_1} = x_1, \ldots, X_{t_n} = x_n) = P(X_{t_{n+1}} \in B \mid X_{t_n} = x_n).
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-44" class="theorem"><strong>Theorem 1.3  </strong></span>If <span class="math inline">\(E\)</span> is discrete and <span class="math inline">\(T = \mathbb{N}\)</span>, the Markov property in the previous definition is equivalent to the following:
<span class="math display">\[
\forall ~n \in \mathbb{N}, ~\forall ~x_0, \ldots, x_{n+1} \in E: P(X_0 = x_0, \ldots, X_n = x_n) &gt; 0, \text{ we have }
\]</span>
<span class="math display">\[
P(X_{n+1} = x_{n+1} \mid X_0 = x_0, \ldots, X_n = x_n) = P(X_{n+1} = x_{n+1} \mid X_n = x_n).
\]</span></p>
</div>
<p><span class="math inline">\(\,\)</span></p>
<div class="remark">
<p><span id="unlabeled-div-45" class="remark"><em>Remark</em>. </span>Markov processes, like any stochastic processes, are classified according to the nature of the state space <span class="math inline">\(E\)</span> and the parameter space <span class="math inline">\(T\)</span>. A special class of Markov processes are <strong>Markov Chains</strong> (M.C.): Markov processes with <strong>discrete</strong> state space <span class="math inline">\(E\)</span>.</p>
<p>Thus, a Markov chain can be interpreted as a stochastic process whose evolution can be seen as a series of transitions between fixed values having the property that the probability distribution of the future state, given that the process is currently in a certain state, depends only on that state and not on how the process arrived there. Markov chains are classified as either <strong>discrete-time</strong> or <strong>continuous-time</strong>.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sde.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
